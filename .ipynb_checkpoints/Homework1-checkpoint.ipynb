{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Elasticsearch\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open connection to elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'ap_dataset2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host':'localhost','port':9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch([{'host': 'localhost', 'port': 9200}])>"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1262,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = {\n",
    "    \"settings\" : {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 1,\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords_path\": \"my_stoplist.txt\"\n",
    "                },\n",
    "                \"english_stemmer\":{\n",
    "                    \"type\": \"stemmer\",\n",
    "                    \"Language\": \"english\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"stopped\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\",\n",
    "                        \"english_stemmer\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "      }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"fielddata\": True,\n",
    "                \"analyzer\": \"stopped\",\n",
    "                \"index_options\": \"positions\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1263,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = es.indices.create(index = index_name, body = request_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'ap_dataset2'}"
      ]
     },
     "execution_count": 1264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open files and parse documents.  Add to index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDocuments(file):\n",
    "    in_text = False\n",
    "    ind = 1\n",
    "    doc_text = ''\n",
    "    doc_id = ''\n",
    "    for line in file:\n",
    "        str_arr = line.split()\n",
    "        if(len(str_arr) > 0):\n",
    "            if(str_arr[0] == '<DOCNO>'):\n",
    "                doc_id = str_arr[1]\n",
    "            if(str_arr[0] == '</TEXT>'):\n",
    "                in_text=False\n",
    "            if(in_text):\n",
    "                doc_text = doc_text + line\n",
    "            if(str_arr[0] == '<TEXT>'):\n",
    "                in_text = True\n",
    "            if(str_arr[0] == '</DOC>' and doc_id != ''):\n",
    "                docExample = {\"text\":doc_text}\n",
    "                res = es.index(index=index_name,id=doc_id, body=docExample)\n",
    "                doc_id = ''\n",
    "                doc_text = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_prefix = 'AP_DATA/ap89_collection/'\n",
    "list_folder_names = os.listdir('/Users/celiasherry/Documents/NE/Spring2020/IR/HW/AP_DATA/ap89_collection')\n",
    "list_folder_names.remove('.DS_Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 1267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_folder_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFolders(list_folder_names):\n",
    "    for folder in list_folder_names:\n",
    "        file = open(path_prefix + folder, 'r', encoding = \"ISO-8859-1\")\n",
    "        addDocuments(file)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseFolders(list_folder_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check number of documents in index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1579742202 01:16:42 84678\\n'"
      ]
     },
     "execution_count": 1272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.refresh(index=index_name)\n",
    "es.cat.count(index=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'token': 'dog',\n",
       "   'start_offset': 0,\n",
       "   'end_offset': 3,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 0},\n",
       "  {'token': 'brown',\n",
       "   'start_offset': 4,\n",
       "   'end_offset': 9,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 1},\n",
       "  {'token': 'cow',\n",
       "   'start_offset': 10,\n",
       "   'end_offset': 14,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 2},\n",
       "  {'token': 'movi',\n",
       "   'start_offset': 15,\n",
       "   'end_offset': 21,\n",
       "   'type': '<ALPHANUM>',\n",
       "   'position': 3}]}"
      ]
     },
     "execution_count": 1271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Dog brown cows movies\"\n",
    "request = {\n",
    "    'field':'text',\n",
    "    'text': query\n",
    "}\n",
    "es.indices.analyze(index=index_name, body=request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Term frequency in document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'14,112': {'term_freq': 1,\n",
       "  'tokens': [{'position': 195, 'start_offset': 1129, 'end_offset': 1135}]},\n",
       " '14,158': {'term_freq': 1,\n",
       "  'tokens': [{'position': 204, 'start_offset': 1181, 'end_offset': 1187}]},\n",
       " '14,410': {'term_freq': 2,\n",
       "  'tokens': [{'position': 61, 'start_offset': 377, 'end_offset': 383},\n",
       "   {'position': 179, 'start_offset': 1035, 'end_offset': 1041}]},\n",
       " '21': {'term_freq': 1,\n",
       "  'tokens': [{'position': 141, 'start_offset': 808, 'end_offset': 810}]},\n",
       " '26': {'term_freq': 1,\n",
       "  'tokens': [{'position': 81, 'start_offset': 473, 'end_offset': 475}]},\n",
       " '33': {'term_freq': 1,\n",
       "  'tokens': [{'position': 83, 'start_offset': 480, 'end_offset': 482}]},\n",
       " '400': {'term_freq': 1,\n",
       "  'tokens': [{'position': 168, 'start_offset': 965, 'end_offset': 968}]},\n",
       " 'afternoon': {'term_freq': 1,\n",
       "  'tokens': [{'position': 137, 'start_offset': 780, 'end_offset': 789}]},\n",
       " 'ag': {'term_freq': 1,\n",
       "  'tokens': [{'position': 79, 'start_offset': 460, 'end_offset': 464}]},\n",
       " 'area': {'term_freq': 2,\n",
       "  'tokens': [{'position': 89, 'start_offset': 508, 'end_offset': 512},\n",
       "   {'position': 245, 'start_offset': 1406, 'end_offset': 1410}]},\n",
       " 'ascent': {'term_freq': 1,\n",
       "  'tokens': [{'position': 289, 'start_offset': 1660, 'end_offset': 1666}]},\n",
       " 'author': {'term_freq': 1,\n",
       "  'tokens': [{'position': 97, 'start_offset': 555, 'end_offset': 566}]},\n",
       " 'bodi': {'term_freq': 1,\n",
       "  'tokens': [{'position': 161, 'start_offset': 920, 'end_offset': 924}]},\n",
       " 'campground': {'term_freq': 1,\n",
       "  'tokens': [{'position': 264, 'start_offset': 1510, 'end_offset': 1520}]},\n",
       " 'cap': {'term_freq': 2,\n",
       "  'tokens': [{'position': 173, 'start_offset': 994, 'end_offset': 997},\n",
       "   {'position': 188, 'start_offset': 1095, 'end_offset': 1098}]},\n",
       " 'climber': {'term_freq': 2,\n",
       "  'tokens': [{'position': 15, 'start_offset': 92, 'end_offset': 99},\n",
       "   {'position': 272, 'start_offset': 1563, 'end_offset': 1571}]},\n",
       " 'columbia': {'term_freq': 1,\n",
       "  'tokens': [{'position': 176, 'start_offset': 1016, 'end_offset': 1024}]},\n",
       " 'commerci': {'term_freq': 1,\n",
       "  'tokens': [{'position': 149, 'start_offset': 852, 'end_offset': 862}]},\n",
       " 'companion': {'term_freq': 1,\n",
       "  'tokens': [{'position': 103, 'start_offset': 594, 'end_offset': 604}]},\n",
       " 'creek': {'term_freq': 1,\n",
       "  'tokens': [{'position': 263, 'start_offset': 1504, 'end_offset': 1509}]},\n",
       " 'crest': {'term_freq': 1,\n",
       "  'tokens': [{'position': 177, 'start_offset': 1025, 'end_offset': 1030}]},\n",
       " 'cy': {'term_freq': 1,\n",
       "  'tokens': [{'position': 33, 'start_offset': 214, 'end_offset': 216}]},\n",
       " 'dead': {'term_freq': 3,\n",
       "  'tokens': [{'position': 20, 'start_offset': 124, 'end_offset': 128},\n",
       "   {'position': 127, 'start_offset': 728, 'end_offset': 732},\n",
       "   {'position': 159, 'start_offset': 909, 'end_offset': 913}]},\n",
       " 'death': {'term_freq': 1,\n",
       "  'tokens': [{'position': 268, 'start_offset': 1535, 'end_offset': 1540}]},\n",
       " 'descend': {'term_freq': 1,\n",
       "  'tokens': [{'position': 259, 'start_offset': 1480, 'end_offset': 1490}]},\n",
       " 'descent': {'term_freq': 1,\n",
       "  'tokens': [{'position': 282, 'start_offset': 1608, 'end_offset': 1615}]},\n",
       " 'di': {'term_freq': 2,\n",
       "  'tokens': [{'position': 105, 'start_offset': 609, 'end_offset': 613},\n",
       "   {'position': 211, 'start_offset': 1227, 'end_offset': 1231}]},\n",
       " 'earlier': {'term_freq': 1,\n",
       "  'tokens': [{'position': 221, 'start_offset': 1280, 'end_offset': 1287}]},\n",
       " 'eastern': {'term_freq': 1,\n",
       "  'tokens': [{'position': 249, 'start_offset': 1428, 'end_offset': 1435}]},\n",
       " 'en': {'term_freq': 1,\n",
       "  'tokens': [{'position': 152, 'start_offset': 878, 'end_offset': 880}]},\n",
       " 'even': {'term_freq': 1,\n",
       "  'tokens': [{'position': 166, 'start_offset': 951, 'end_offset': 958}]},\n",
       " 'feet': {'term_freq': 4,\n",
       "  'tokens': [{'position': 169, 'start_offset': 969, 'end_offset': 973},\n",
       "   {'position': 180, 'start_offset': 1042, 'end_offset': 1046},\n",
       "   {'position': 196, 'start_offset': 1136, 'end_offset': 1140},\n",
       "   {'position': 205, 'start_offset': 1188, 'end_offset': 1192}]},\n",
       " 'ferri': {'term_freq': 1,\n",
       "  'tokens': [{'position': 37, 'start_offset': 242, 'end_offset': 249}]},\n",
       " 'fly': {'term_freq': 1,\n",
       "  'tokens': [{'position': 58, 'start_offset': 359, 'end_offset': 365}]},\n",
       " 'foot': {'term_freq': 1,\n",
       "  'tokens': [{'position': 62, 'start_offset': 384, 'end_offset': 388}]},\n",
       " 'found': {'term_freq': 4,\n",
       "  'tokens': [{'position': 19, 'start_offset': 118, 'end_offset': 123},\n",
       "   {'position': 47, 'start_offset': 303, 'end_offset': 308},\n",
       "   {'position': 164, 'start_offset': 936, 'end_offset': 941},\n",
       "   {'position': 239, 'start_offset': 1362, 'end_offset': 1367}]},\n",
       " 'four': {'term_freq': 4,\n",
       "  'tokens': [{'position': 38, 'start_offset': 250, 'end_offset': 254},\n",
       "   {'position': 77, 'start_offset': 451, 'end_offset': 455},\n",
       "   {'position': 115, 'start_offset': 661, 'end_offset': 665},\n",
       "   {'position': 271, 'start_offset': 1558, 'end_offset': 1562}]},\n",
       " 'fridai': {'term_freq': 3,\n",
       "  'tokens': [{'position': 11, 'start_offset': 71, 'end_offset': 77},\n",
       "   {'position': 136, 'start_offset': 773, 'end_offset': 779},\n",
       "   {'position': 154, 'start_offset': 887, 'end_offset': 893}]},\n",
       " 'glacier': {'term_freq': 1,\n",
       "  'tokens': [{'position': 286, 'start_offset': 1636, 'end_offset': 1644}]},\n",
       " 'ground': {'term_freq': 1,\n",
       "  'tokens': [{'position': 41, 'start_offset': 265, 'end_offset': 271}]},\n",
       " 'group': {'term_freq': 1,\n",
       "  'tokens': [{'position': 39, 'start_offset': 255, 'end_offset': 261}]},\n",
       " 'helicopt': {'term_freq': 3,\n",
       "  'tokens': [{'position': 36, 'start_offset': 230, 'end_offset': 241},\n",
       "   {'position': 55, 'start_offset': 339, 'end_offset': 349},\n",
       "   {'position': 150, 'start_offset': 863, 'end_offset': 873}]},\n",
       " 'hentg': {'term_freq': 6,\n",
       "  'tokens': [{'position': 34, 'start_offset': 217, 'end_offset': 224},\n",
       "   {'position': 90, 'start_offset': 514, 'end_offset': 521},\n",
       "   {'position': 120, 'start_offset': 690, 'end_offset': 697},\n",
       "   {'position': 174, 'start_offset': 999, 'end_offset': 1006},\n",
       "   {'position': 206, 'start_offset': 1197, 'end_offset': 1204},\n",
       "   {'position': 292, 'start_offset': 1683, 'end_offset': 1690}]},\n",
       " 'highest': {'term_freq': 1,\n",
       "  'tokens': [{'position': 184, 'start_offset': 1066, 'end_offset': 1073}]},\n",
       " 'hypothermia': {'term_freq': 1,\n",
       "  'tokens': [{'position': 213, 'start_offset': 1235, 'end_offset': 1246}]},\n",
       " 'involv': {'term_freq': 1,\n",
       "  'tokens': [{'position': 144, 'start_offset': 823, 'end_offset': 831}]},\n",
       " 'ipsut': {'term_freq': 1,\n",
       "  'tokens': [{'position': 262, 'start_offset': 1498, 'end_offset': 1503}]},\n",
       " 'lake': {'term_freq': 1,\n",
       "  'tokens': [{'position': 87, 'start_offset': 497, 'end_offset': 501}]},\n",
       " 'liberti': {'term_freq': 2,\n",
       "  'tokens': [{'position': 172, 'start_offset': 986, 'end_offset': 993},\n",
       "   {'position': 187, 'start_offset': 1087, 'end_offset': 1094}]},\n",
       " 'man': {'term_freq': 3,\n",
       "  'tokens': [{'position': 53, 'start_offset': 332, 'end_offset': 335},\n",
       "   {'position': 209, 'start_offset': 1214, 'end_offset': 1217},\n",
       "   {'position': 257, 'start_offset': 1469, 'end_offset': 1472}]},\n",
       " \"man'\": {'term_freq': 1,\n",
       "  'tokens': [{'position': 160, 'start_offset': 914, 'end_offset': 919}]},\n",
       " 'men': {'term_freq': 4,\n",
       "  'tokens': [{'position': 71, 'start_offset': 428, 'end_offset': 431},\n",
       "   {'position': 78, 'start_offset': 456, 'end_offset': 459},\n",
       "   {'position': 95, 'start_offset': 542, 'end_offset': 545},\n",
       "   {'position': 130, 'start_offset': 745, 'end_offset': 748}]},\n",
       " 'miss': {'term_freq': 5,\n",
       "  'tokens': [{'position': 14, 'start_offset': 84, 'end_offset': 91},\n",
       "   {'position': 52, 'start_offset': 324, 'end_offset': 331},\n",
       "   {'position': 110, 'start_offset': 632, 'end_offset': 639},\n",
       "   {'position': 129, 'start_offset': 737, 'end_offset': 744},\n",
       "   {'position': 256, 'start_offset': 1461, 'end_offset': 1468}]},\n",
       " 'morn': {'term_freq': 1,\n",
       "  'tokens': [{'position': 291, 'start_offset': 1674, 'end_offset': 1681}]},\n",
       " 'mount': {'term_freq': 2,\n",
       "  'tokens': [{'position': 8, 'start_offset': 54, 'end_offset': 59},\n",
       "   {'position': 25, 'start_offset': 152, 'end_offset': 157}]},\n",
       " 'mountain': {'term_freq': 1,\n",
       "  'tokens': [{'position': 45, 'start_offset': 289, 'end_offset': 297}]},\n",
       " \"mountain'\": {'term_freq': 2,\n",
       "  'tokens': [{'position': 183, 'start_offset': 1055, 'end_offset': 1065},\n",
       "   {'position': 285, 'start_offset': 1625, 'end_offset': 1635}]},\n",
       " 'name': {'term_freq': 1,\n",
       "  'tokens': [{'position': 112, 'start_offset': 648, 'end_offset': 653}]},\n",
       " 'nation': {'term_freq': 1,\n",
       "  'tokens': [{'position': 27, 'start_offset': 166, 'end_offset': 174}]},\n",
       " 'northwest': {'term_freq': 1,\n",
       "  'tokens': [{'position': 191, 'start_offset': 1107, 'end_offset': 1116}]},\n",
       " 'notifi': {'term_freq': 2,\n",
       "  'tokens': [{'position': 96, 'start_offset': 546, 'end_offset': 554},\n",
       "   {'position': 133, 'start_offset': 758, 'end_offset': 766}]},\n",
       " 'offici': {'term_freq': 1,\n",
       "  'tokens': [{'position': 29, 'start_offset': 180, 'end_offset': 189}]},\n",
       " 'park': {'term_freq': 3,\n",
       "  'tokens': [{'position': 0, 'start_offset': 3, 'end_offset': 7},\n",
       "   {'position': 28, 'start_offset': 175, 'end_offset': 179},\n",
       "   {'position': 31, 'start_offset': 199, 'end_offset': 203}]},\n",
       " 'parka': {'term_freq': 2,\n",
       "  'tokens': [{'position': 220, 'start_offset': 1274, 'end_offset': 1279},\n",
       "   {'position': 229, 'start_offset': 1317, 'end_offset': 1322}]},\n",
       " 'part': {'term_freq': 1,\n",
       "  'tokens': [{'position': 65, 'start_offset': 397, 'end_offset': 401}]},\n",
       " 'parti': {'term_freq': 1,\n",
       "  'tokens': [{'position': 75, 'start_offset': 442, 'end_offset': 447}]},\n",
       " 'partner': {'term_freq': 1,\n",
       "  'tokens': [{'position': 17, 'start_offset': 106, 'end_offset': 113}]},\n",
       " 'peak': {'term_freq': 2,\n",
       "  'tokens': [{'position': 63, 'start_offset': 389, 'end_offset': 393},\n",
       "   {'position': 253, 'start_offset': 1448, 'end_offset': 1452}]},\n",
       " 'peopl': {'term_freq': 1,\n",
       "  'tokens': [{'position': 142, 'start_offset': 811, 'end_offset': 817}]},\n",
       " 'pick': {'term_freq': 1,\n",
       "  'tokens': [{'position': 156, 'start_offset': 897, 'end_offset': 901}]},\n",
       " 'point': {'term_freq': 2,\n",
       "  'tokens': [{'position': 185, 'start_offset': 1074, 'end_offset': 1079},\n",
       "   {'position': 200, 'start_offset': 1160, 'end_offset': 1165}]},\n",
       " 'probabl': {'term_freq': 1,\n",
       "  'tokens': [{'position': 210, 'start_offset': 1218, 'end_offset': 1226}]},\n",
       " 'rainier': {'term_freq': 2,\n",
       "  'tokens': [{'position': 9, 'start_offset': 60, 'end_offset': 67},\n",
       "   {'position': 26, 'start_offset': 158, 'end_offset': 165}]},\n",
       " 'ranger': {'term_freq': 1,\n",
       "  'tokens': [{'position': 1, 'start_offset': 8, 'end_offset': 15}]},\n",
       " 'rel': {'term_freq': 1,\n",
       "  'tokens': [{'position': 124, 'start_offset': 711, 'end_offset': 720}]},\n",
       " 'releas': {'term_freq': 1,\n",
       "  'tokens': [{'position': 119, 'start_offset': 680, 'end_offset': 688}]},\n",
       " 'report': {'term_freq': 1,\n",
       "  'tokens': [{'position': 266, 'start_offset': 1524, 'end_offset': 1530}]},\n",
       " 'rout': {'term_freq': 1,\n",
       "  'tokens': [{'position': 153, 'start_offset': 881, 'end_offset': 886}]},\n",
       " 'search': {'term_freq': 5,\n",
       "  'tokens': [{'position': 4, 'start_offset': 31, 'end_offset': 39},\n",
       "   {'position': 68, 'start_offset': 409, 'end_offset': 415},\n",
       "   {'position': 147, 'start_offset': 839, 'end_offset': 845},\n",
       "   {'position': 242, 'start_offset': 1386, 'end_offset': 1394},\n",
       "   {'position': 247, 'start_offset': 1415, 'end_offset': 1423}]},\n",
       " 'searcher': {'term_freq': 1,\n",
       "  'tokens': [{'position': 42, 'start_offset': 272, 'end_offset': 281}]},\n",
       " 'side': {'term_freq': 2,\n",
       "  'tokens': [{'position': 192, 'start_offset': 1117, 'end_offset': 1121},\n",
       "   {'position': 250, 'start_offset': 1436, 'end_offset': 1440}]},\n",
       " 'sign': {'term_freq': 1,\n",
       "  'tokens': [{'position': 49, 'start_offset': 312, 'end_offset': 316}]},\n",
       " 'ski': {'term_freq': 3,\n",
       "  'tokens': [{'position': 227, 'start_offset': 1307, 'end_offset': 1311},\n",
       "   {'position': 235, 'start_offset': 1345, 'end_offset': 1349},\n",
       "   {'position': 277, 'start_offset': 1589, 'end_offset': 1593}]},\n",
       " 'spokesman': {'term_freq': 1,\n",
       "  'tokens': [{'position': 32, 'start_offset': 204, 'end_offset': 213}]},\n",
       " 'start': {'term_freq': 1,\n",
       "  'tokens': [{'position': 287, 'start_offset': 1646, 'end_offset': 1653}]},\n",
       " 'success': {'term_freq': 1,\n",
       "  'tokens': [{'position': 201, 'start_offset': 1166, 'end_offset': 1173}]},\n",
       " 'summit': {'term_freq': 4,\n",
       "  'tokens': [{'position': 6, 'start_offset': 44, 'end_offset': 50},\n",
       "   {'position': 24, 'start_offset': 144, 'end_offset': 150},\n",
       "   {'position': 199, 'start_offset': 1152, 'end_offset': 1158},\n",
       "   {'position': 244, 'start_offset': 1399, 'end_offset': 1405}]},\n",
       " 'sundai': {'term_freq': 1,\n",
       "  'tokens': [{'position': 290, 'start_offset': 1667, 'end_offset': 1673}]},\n",
       " 'survivor': {'term_freq': 1,\n",
       "  'tokens': [{'position': 241, 'start_offset': 1376, 'end_offset': 1385}]},\n",
       " 'taho': {'term_freq': 1,\n",
       "  'tokens': [{'position': 88, 'start_offset': 502, 'end_offset': 507}]},\n",
       " 'taken': {'term_freq': 1,\n",
       "  'tokens': [{'position': 217, 'start_offset': 1260, 'end_offset': 1265}]},\n",
       " 'third': {'term_freq': 1,\n",
       "  'tokens': [{'position': 198, 'start_offset': 1146, 'end_offset': 1151}]},\n",
       " 'thursdai': {'term_freq': 3,\n",
       "  'tokens': [{'position': 98, 'start_offset': 567, 'end_offset': 575},\n",
       "   {'position': 165, 'start_offset': 942, 'end_offset': 950},\n",
       "   {'position': 269, 'start_offset': 1541, 'end_offset': 1549}]},\n",
       " 'ti': {'term_freq': 2,\n",
       "  'tokens': [{'position': 223, 'start_offset': 1292, 'end_offset': 1296},\n",
       "   {'position': 232, 'start_offset': 1333, 'end_offset': 1337}]},\n",
       " 'two': {'term_freq': 2,\n",
       "  'tokens': [{'position': 70, 'start_offset': 424, 'end_offset': 427},\n",
       "   {'position': 94, 'start_offset': 538, 'end_offset': 541}]},\n",
       " 'volunt': {'term_freq': 1,\n",
       "  'tokens': [{'position': 3, 'start_offset': 20, 'end_offset': 30}]}}"
      ]
     },
     "execution_count": 1302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term = 'rainier'\n",
    "doc = 'AP890513-0001'\n",
    "#es.termvectors(index=index_name, id=doc, fields='text')['term_vectors']['text']['terms'].get(term,{})\n",
    "\n",
    "es.termvectors(index=index_name,id=doc,fields='text')['term_vectors']['text']['terms']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_freq': 31,\n",
       " 'ttf': 48,\n",
       " 'term_freq': 2,\n",
       " 'tokens': [{'position': 9, 'start_offset': 60, 'end_offset': 67},\n",
       "  {'position': 26, 'start_offset': 158, 'end_offset': 165}]}"
      ]
     },
     "execution_count": 1275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.termvectors(index=index_name, id=doc, fields='text',term_statistics=True)['term_vectors']['text']['terms'] \\\n",
    ".get(term,{})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vocab Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192916"
      ]
     },
     "execution_count": 1276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request = {\n",
    "    \"aggs\":{\n",
    "        \"vocabSize\": {\n",
    "            \"cardinality\": {\n",
    "                \"field\": \"text\"},\n",
    "        }\n",
    "    }\n",
    "    , \"size\":0}\n",
    "es.search(index=index_name, body=request)['aggregations']['vocabSize']['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Document Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 1277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id = 'AP890513-0003'\n",
    "sum(map(lambda t: t['term_freq'], \n",
    "        es.termvectors(index=index_name,id=doc_id, fields='text',term_statistics=True)['term_vectors']['text']['terms'].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Average Document Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248.65458026544385"
      ]
     },
     "execution_count": 1278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.termvectors(index=index_name,id='AP891220-0113', fields='text', field_statistics=True)['term_vectors']['text']['field_statistics']['sum_ttf']/es.termvectors(index=index_name,id='AP891220-0113', fields='text', field_statistics=True)['term_vectors']['text']['field_statistics']['doc_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ES built in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create file to write results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"ES_builtin_output_file.txt\"\n",
    "f = open(output_file, \"w+\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse results and write results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(query_num, res, file_output):\n",
    "    i = 1\n",
    "    for doc in res:\n",
    "        if i > 1000:\n",
    "            break\n",
    "        _id = doc['_id']\n",
    "        _score = doc['_score']\n",
    "        rank = i\n",
    "        i += 1\n",
    "        file_output.write(str(query_num) + \" Q0 \" + str(_id) + \" \" + str(rank) + \" \" + str(_score) + \" Exp\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open query file and run search on each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1462,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"AP_DATA/query_desc.51-100.short.txt\", 'r')\n",
    "f_output = open(output_file, \"w\")\n",
    "for line in file:\n",
    "    query_number = line.split('.')[0]\n",
    "    query_text = line[6:]\n",
    "    request = {\n",
    "        \"sort\":[{\n",
    "            \"_score\" : {\n",
    "                \"order\":\"desc\"\n",
    "            }}\n",
    "        ],\n",
    "        \"query\":{\n",
    "            \"match\":{\n",
    "                \"text\":{\n",
    "                    \"query\":query_text,\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'size': 10000\n",
    "    }\n",
    "    res = es.search(index=index_name, body=request)['hits']['hits']\n",
    "    write_to_file(query_number, res, f_output)\n",
    "file.close()\n",
    "f_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okapi TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define method to get average document length of index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_doc_length():\n",
    "    return es.termvectors(index=index_name,id='AP891220-0113', fields='text', field_statistics=True)['term_vectors']['text']['field_statistics']['sum_ttf']/es.termvectors(index=index_name,id='AP891220-0113', fields='text', field_statistics=True)['term_vectors']['text']['field_statistics']['doc_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define method to get document length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_length(doc_id):\n",
    "    return sum(map(lambda t: t['term_freq'], \n",
    "            es.termvectors(index=index_name,id=doc_id, fields='text',term_statistics=True)['term_vectors']['text']['terms'].values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get nested dictionary of each document and term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_vectors(batch):\n",
    "    doc_dict = {}\n",
    "    doc_length_dict = {}\n",
    "    request = {\n",
    "        \"ids\": batch,\n",
    "        \"parameters\": {\n",
    "            \"fields\": [\n",
    "                \"text\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    res = es.mtermvectors(index=index_name, body=request)['docs']\n",
    "    for doc in res:\n",
    "        doc_id = doc['_id']\n",
    "        doc_length_dict[doc_id] = get_doc_length(doc_id)\n",
    "        tv = doc['term_vectors']\n",
    "        if len(tv) != 0:\n",
    "            doc_dict[doc_id] = {}\n",
    "            term_freq = tv['text']['terms']\n",
    "            for term in term_freq:\n",
    "                f = term_freq.get(term, {})['term_freq']\n",
    "                doc_dict[doc_id][term] = f\n",
    "    return doc_dict, doc_length_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get nested dictionary for all documents in index, batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import partition_all\n",
    "\n",
    "def partition_docs(docs):\n",
    "    batch_size = 1000\n",
    "    partitions = list(partition_all(batch_size, docs))\n",
    "    doc_dict = {}\n",
    "    doc_length = {}\n",
    "    for partition in partitions:\n",
    "        new_dict, doc_length_dict = get_term_vectors(partition)\n",
    "        doc_dict.update(new_dict)\n",
    "        doc_length.update(doc_length_dict)\n",
    "    return doc_dict, doc_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use elasticsearch to query for documents that contain at least one query word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_with_query_words(query):\n",
    "    page = es.search(\n",
    "    index = index_name,\n",
    "    scroll = '1m',\n",
    "    size = 1000,\n",
    "    body = {\n",
    "        \"_source\":\"_id\",\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "              \"text\":{\n",
    "                \"query\": query,\n",
    "                \"operator\" : \"or\"\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        })\n",
    "    sid = page['_scroll_id']\n",
    "    scroll_size = page['hits']['total']['value']\n",
    "    doc_ids = []\n",
    "    for doc in page['hits']['hits']:\n",
    "        doc_ids.append(doc['_id'])\n",
    "\n",
    "    while (scroll_size > 0):\n",
    "        page = es.scroll(scroll_id=sid, scroll='1m')\n",
    "        scroll_size = len(page['hits']['hits'])\n",
    "        for doc in page['hits']['hits']:\n",
    "            doc_ids.append(doc['_id'])\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tokens from query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(query):\n",
    "    token_list = []\n",
    "    request = {\n",
    "        'field':'text',\n",
    "        'text': query\n",
    "    }\n",
    "    res = es.indices.analyze(index=index_name, body=request)['tokens']\n",
    "    # Get list of keywords in query\n",
    "    for token in res:\n",
    "        token_list.append(token['token'])\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort dictionary and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def write_to_file_okapi(query_num, score_dict, file_output):\n",
    "    sort = sorted(score_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    i = 1\n",
    "    for item in sort:\n",
    "        if i > 1000:\n",
    "            break\n",
    "        _id = item[0]\n",
    "        _score = item[1]\n",
    "        rank = i\n",
    "        i += 1\n",
    "        file_output.write(str(query_num) + \" Q0 \" + str(_id) + \" \" + str(rank) + \" \" + str(_score) + \" Exp\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through queries and write output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_okapi_tf():\n",
    "    okapi_output_file = \"Okapi_TF_output_file.txt\"\n",
    "    f_output = open(okapi_output_file, \"w+\")\n",
    "    file = open(\"AP_DATA/EC2_query_desc.51-100.short.txt\", 'r')\n",
    "    \n",
    "    for line in file:\n",
    "        query_number = line.split('.')[0]\n",
    "        query_text = line[6:]\n",
    "        \n",
    "        tokens = get_tokens(query_text)\n",
    "        avg_doc_length = get_avg_doc_length()\n",
    "        \n",
    "        score_dict = {}\n",
    "        \n",
    "        for term in tokens:\n",
    "            \n",
    "            doc_ids = get_doc_with_query_words(term)\n",
    "            master_doc_dict, doc_length_dict = partition_docs(doc_ids)\n",
    "            \n",
    "            for doc in doc_ids:\n",
    "                if term in master_doc_dict[doc].keys():\n",
    "                    tf = master_doc_dict[doc][term]\n",
    "                    if doc in score_dict.keys():\n",
    "                        score_dict[doc] = score_dict[doc] + (tf/(((doc_length_dict[doc]/avg_doc_length)*1.5)+.5+tf))\n",
    "                    else:\n",
    "                        score_dict[doc] = (tf/(((doc_length_dict[doc]/avg_doc_length)*1.5)+.5+tf))\n",
    "                    \n",
    "        write_to_file_okapi(query_number, score_dict, f_output)\n",
    "                \n",
    "    file.close()\n",
    "    f_output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Okapi TF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allegations corrupt government   \n",
      "\n",
      "['alleg', 'corrupt', 'govern']\n",
      "weather caused death\n",
      "\n",
      "['weather', 'caus', 'death']\n",
      "prediction prime lending rate move \n",
      "\n",
      "['predict', 'prime', 'lend', 'rate', 'move']\n",
      "attack guerrilla border military\n",
      "\n",
      "['attack', 'guerrilla', 'border', 'militari']\n",
      "politically motivated hostage-taking hostage\n",
      "\n",
      "['polit', 'motiv', 'hostag', 'take', 'hostag']\n",
      "military coup d'etat  \n",
      "\n",
      "['militari', 'coup', \"d'etat\"]\n",
      "supporters National Rifle Association NRA\n",
      "\n",
      "['support', 'nation', 'rifl', 'associ', 'nra']\n",
      "Iran-Contra Affair  \n",
      "\n",
      "['iran', 'contra', 'affair']\n",
      "rail strike \n",
      "\n",
      "['rail', 'strike']\n",
      "poaching wildlife poach\n",
      "\n",
      "['poach', 'wildlif', 'poach']\n",
      "signing contract preliminary agreement launch commercial satellite  \n",
      "\n",
      "['sign', 'contract', 'preliminari', 'agreement', 'launch', 'commerci', 'satellit']\n",
      "current criminal actions against officers failed U.S. financial institution\n",
      "\n",
      "['current', 'crimin', 'action', 'offic', 'fail', 'u.', 'financi', 'institut']\n",
      "crime computer\n",
      "\n",
      "['crime', 'comput']\n",
      "non-communist industrialized states regulate transfer high-tech goods \"dual-use\" technologies  \n",
      "\n",
      "['non', 'communist', 'industri', 'state', 'regul', 'transfer', 'high', 'tech', 'good', 'technolog']\n",
      "investment OPEC member state \"downstream\" operation  \n",
      "\n",
      "['invest', 'opec', 'member', 'state', 'downstream', 'oper']\n",
      "Israel Iran-Contra Affair  \n",
      "\n",
      "['israel', 'iran', 'contra', 'affair']\n",
      "computer crime solving \n",
      "\n",
      "['comput', 'crime', 'solv']\n",
      "safety manufacturing employees installation workers fine-diameter fibers insulation  \n",
      "\n",
      "['safeti', 'manufactur', 'employe', 'instal', 'worker', 'fine', 'diamet', 'fiber', 'insul']\n",
      "MCI Bell System breakup \n",
      "\n",
      "['mci', 'bell', 'system', 'breakup']\n",
      "fiber optics technology in use   \n",
      "\n",
      "['fiber', 'optic', 'technolog']\n",
      "produce fiber optics equipment  \n",
      "\n",
      "['produc', 'fiber', 'optic', 'equip']\n",
      "standards performance determine salary levels incentive pay seniority longevity  \n",
      "\n",
      "['standard', 'perform', 'determin', 'salari', 'level', 'incent', 'pai', 'senior', 'longev']\n",
      "1988 presidential candidate president\n",
      "\n",
      "['1988', 'presidenti', 'candid', 'presid']\n",
      "machine translation \n",
      "\n",
      "['machin', 'translat']\n",
      "acquisition U.S. Army specified advanced weapons systems increased\n",
      "['acquisit', 'u.', 'armi', 'specifi', 'advanc', 'weapon', 'system', 'increas']\n"
     ]
    }
   ],
   "source": [
    "get_okapi_tf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get total number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_num_documents():\n",
    "    return es.count(index=index_name).get('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get number of documents that contain term m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_w(term_list):\n",
    "    term_dict = {}\n",
    "    doc_count = get_total_num_documents()\n",
    "    for word in term_list:\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"term\" : {\n",
    "                    \"text\": word\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        df = es.count(index=index_name, body=query)['count']\n",
    "        if df != 0:\n",
    "            term_dict[word] = math.log10(doc_count/df)\n",
    "    return term_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get TF-IDF for document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf_for_doc(keyword_list, term_dict, avg_doc_length, token_dict, doc_length):\n",
    "    denom = (1.5 * (doc_length/avg_doc_length)) + .5\n",
    "    okapi_tf = 0\n",
    "    for token in keyword_list:\n",
    "        if token in term_dict.keys():\n",
    "            tf = term_dict[token]\n",
    "            okapi_tf_temp = (tf / (tf + denom)) * token_dict[token]\n",
    "            okapi_tf = okapi_tf + okapi_tf_temp\n",
    "    return okapi_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through queries and write to output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf():\n",
    "    tf_output_file = \"TF_IDF_output_file.txt\"\n",
    "    f_output = open(tf_output_file, \"w+\")\n",
    "    file = open(\"AP_DATA/EC2_query_desc.51-100.short.txt\", 'r')\n",
    "    \n",
    "    for line in file:\n",
    "        query_number = line.split('.')[0]\n",
    "        query_text = line[6:]\n",
    "        \n",
    "        tokens = get_tokens(query_text)\n",
    "        df_multiplier_dict = get_df_w(tokens)\n",
    "        \n",
    "        score_dict = {}\n",
    "        \n",
    "        \n",
    "        for term in tokens:\n",
    "            \n",
    "            doc_ids = get_doc_with_query_words(term)\n",
    "            master_doc_dict, doc_length_dict = partition_docs(doc_ids)\n",
    "            \n",
    "            num_docs = 0\n",
    "            collection_length = 0\n",
    "            for doc in doc_length_dict:\n",
    "                collection_length = collection_length + doc_length_dict[doc]\n",
    "                num_docs = num_docs + 1\n",
    "            avg_doc_length = collection_length/num_docs\n",
    "            \n",
    "            for doc in doc_ids:\n",
    "                if term in master_doc_dict[doc].keys():\n",
    "                    tf = master_doc_dict[doc][term]\n",
    "                    if doc in score_dict.keys():\n",
    "                        score_dict[doc] = score_dict[doc] + ((tf/(((doc_length_dict[doc]/avg_doc_length)*1.5)+.5+tf)) * df_multiplier_dict[term])\n",
    "                    else:\n",
    "                        score_dict[doc] = ((tf/(((doc_length_dict[doc]/avg_doc_length)*1.5)+.5+tf)) * df_multiplier_dict[term])\n",
    "        \n",
    "        write_to_file_okapi(query_number, score_dict, f_output)\n",
    "                \n",
    "    file.close()\n",
    "    f_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acquisit\n",
      "u.\n",
      "armi\n",
      "specifi\n",
      "advanc\n",
      "weapon\n",
      "system\n"
     ]
    }
   ],
   "source": [
    "get_tf_idf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okapi BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get first term in Okapi BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t1_bm25(term_list):\n",
    "    term_dict = {}\n",
    "    doc_count = get_total_num_documents()\n",
    "    for word in term_list:\n",
    "        query = {\n",
    "            \"query\": {\n",
    "                \"term\" : {\n",
    "                    \"text\": word\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        df = es.count(index=index_name, body=query)['count']\n",
    "        if df != 0:\n",
    "            term_dict[word] = math.log10((doc_count+.5)/(df+.5))\n",
    "    return term_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go through queries and write to output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1467,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bm25():\n",
    "    bm25_output_file = \"BM25_output_file.txt\"\n",
    "    f_output = open(bm25_output_file, \"w+\")\n",
    "    file = open(\"AP_DATA/EC2_query_desc.51-100.short.txt\", 'r')\n",
    "    \n",
    "    k1 = 1.2\n",
    "    k2 = 1.2\n",
    "    b = .75\n",
    "    \n",
    "    for line in file:\n",
    "        query_number = line.split('.')[0]\n",
    "        query_text = line[6:]\n",
    "        \n",
    "        tokens = get_tokens(query_text)\n",
    "        avg_doc_length = get_avg_doc_length()\n",
    "        df_multiplier_dict = get_t1_bm25(tokens)\n",
    "        \n",
    "        score_dict = {}\n",
    "        \n",
    "        for term in tokens:\n",
    "            \n",
    "            doc_ids = get_doc_with_query_words(term)\n",
    "            master_doc_dict, doc_length_dict = partition_docs(doc_ids)\n",
    "            \n",
    "            for doc in doc_ids:\n",
    "                if term in master_doc_dict[doc].keys():\n",
    "                    tf = master_doc_dict[doc][term]\n",
    "                    first_term = df_multiplier_dict[term]\n",
    "                    second_term = (tf+k1*tf)/(tf+k1*((1-b)+b*doc_length_dict[doc]/avg_doc_length))\n",
    "                    third_term = (tf+k1*tf) / (tf+k2)\n",
    "                    if doc in score_dict.keys():\n",
    "                        score_dict[doc] = score_dict[doc] + (first_term*second_term*third_term)\n",
    "                    else:\n",
    "                        score_dict[doc] = first_term*second_term*third_term\n",
    "                    \n",
    "        write_to_file_okapi(query_number, score_dict, f_output)\n",
    "                \n",
    "    file.close()\n",
    "    f_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1392,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bm25()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram LM with Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size():\n",
    "    request = {\n",
    "        \"aggs\":{\n",
    "            \"vocabSize\": {\n",
    "                \"cardinality\": {\n",
    "                    \"field\": \"text\"},\n",
    "            }\n",
    "        }\n",
    "        , \"size\":0}\n",
    "    return es.search(index=index_name, body=request)['aggregations']['vocabSize']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_laplace():\n",
    "    laplace_output_file = \"laplace_output_file.txt\"\n",
    "    f_output = open(laplace_output_file, \"w+\")\n",
    "    file = open(\"AP_DATA/EC2_query_desc.51-100.short.txt\", 'r')\n",
    "    \n",
    "    v = get_vocab_size()\n",
    "    \n",
    "    for line in file:\n",
    "        query_number = line.split('.')[0]\n",
    "        query_text = line[6:]\n",
    "        \n",
    "        tokens = get_tokens(query_text)\n",
    "        \n",
    "        doc_ids = get_doc_with_query_words(query_text)\n",
    "        master_doc_dict, doc_length_dict = partition_docs(doc_ids)\n",
    "        \n",
    "        score_dict = {}\n",
    "        \n",
    "        for term in tokens:\n",
    "            \n",
    "            for doc in doc_ids:\n",
    "                if term in master_doc_dict[doc].keys():\n",
    "                    tf = master_doc_dict[doc][term]\n",
    "                    frac = math.log10(float(tf+1)/float(doc_length_dict[doc] + v))\n",
    "                    if doc in score_dict.keys():\n",
    "                        score_dict[doc] = score_dict[doc] + frac\n",
    "                    else:\n",
    "                        score_dict[doc] = frac\n",
    "                else:\n",
    "                    frac2 = math.log10(float(1)/float(doc_length_dict[doc] + v))\n",
    "                    if doc in score_dict.keys():\n",
    "                        score_dict[doc] = score_dict[doc] + frac2\n",
    "                    else:\n",
    "                        score_dict[doc] = frac2\n",
    "                    \n",
    "        write_to_file_okapi(query_number, score_dict, f_output)\n",
    "                \n",
    "    file.close()\n",
    "    f_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1395,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unigram_laplace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram LM with Jelinek-Mercer Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram_jelinek():\n",
    "    jelinek_output_file = \"jelinek_output_file.txt\"\n",
    "    f_output = open(jelinek_output_file, \"w+\")\n",
    "    file = open(\"AP_DATA/EC2_query_desc.51-100.short.txt\", 'r')\n",
    "    \n",
    "    l = 0.8\n",
    "    \n",
    "    for line in file:\n",
    "        query_number = line.split('.')[0]\n",
    "        query_text = line[6:]\n",
    "        \n",
    "        tokens = get_tokens(query_text)\n",
    "        \n",
    "        doc_ids = get_doc_with_query_words(query_text)\n",
    "        master_doc_dict, doc_length_dict = partition_docs(doc_ids)\n",
    "        \n",
    "        total_tf = {}\n",
    "        for doc in master_doc_dict:\n",
    "            for t in master_doc_dict[doc]:\n",
    "                if t in total_tf.keys():\n",
    "                    total_tf[t] = total_tf[t] + master_doc_dict[doc][t]\n",
    "                else:\n",
    "                    total_tf[t] = master_doc_dict[doc][t]\n",
    "                    \n",
    "        collection_length = 0\n",
    "        for doc in doc_length_dict:\n",
    "            collection_length = collection_length + doc_length_dict[doc]\n",
    "            \n",
    "        score_dict = {}\n",
    "        \n",
    "        for term in tokens:\n",
    "            \n",
    "            for doc in doc_ids:\n",
    "                if term in master_doc_dict[doc].keys():\n",
    "                    tf = master_doc_dict[doc][term]\n",
    "                    frac = math.log10(float(l*(tf/doc_length_dict[doc]))/float((1-l)*(total_tf[term]/collection_length)))\n",
    "                    if doc in score_dict.keys():\n",
    "                        score_dict[doc] = score_dict[doc] + frac\n",
    "                    else:\n",
    "                        score_dict[doc] = frac\n",
    "                else:\n",
    "                    frac2 = math.log10(float((1-l)*(total_tf[term]/collection_length)))\n",
    "                    if doc in score_dict.keys():\n",
    "                        score_dict[doc] = score_dict[doc] + frac2\n",
    "                    else:\n",
    "                        score_dict[doc] = frac2\n",
    "                    \n",
    "        write_to_file_okapi(query_number, score_dict, f_output)\n",
    "                \n",
    "    file.close()\n",
    "    f_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1397,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unigram_jelinek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2: Pseudo-relevance Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_docs(query):\n",
    "    page = es.search(\n",
    "    index = index_name,\n",
    "    scroll = '1m',\n",
    "    size = 1000,\n",
    "    body = {\n",
    "        \"_source\":\"_id\",\n",
    "        \"query\": {\n",
    "            \"terms\": {\n",
    "                \"text\": query\n",
    "            }\n",
    "          }\n",
    "        })\n",
    "    sid = page['_scroll_id']\n",
    "    scroll_size = page['hits']['total']['value']\n",
    "    doc_ids = []\n",
    "    for doc in page['hits']['hits']:\n",
    "        doc_ids.append(doc['_id'])\n",
    "\n",
    "    while (scroll_size > 0):\n",
    "        page = es.scroll(scroll_id=sid, scroll='1m')\n",
    "        scroll_size = len(page['hits']['hits'])\n",
    "        for doc in page['hits']['hits']:\n",
    "            doc_ids.append(doc['_id'])\n",
    "    return doc_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ok_tf(tokens, master_doc_dict, doc_ids, doc_length_dict, avg_doc_length):\n",
    "    score_dict = {}\n",
    "    \n",
    "    for term in tokens:\n",
    "        for doc in doc_ids:\n",
    "            if term in master_doc_dict[doc].keys():\n",
    "                tf = master_doc_dict[doc][term]\n",
    "                if doc in score_dict.keys():\n",
    "                    score_dict[doc] = score_dict[doc] + (tf/(((doc_length_dict[doc]/avg_doc_length)*1.5)+.5+tf))\n",
    "                else:\n",
    "                    score_dict[doc] = (tf/(((doc_length_dict[doc]/avg_doc_length)*1.5)+.5+tf))\n",
    "                    \n",
    "    return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1588,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_terms(list_of_doc_ids):\n",
    "    v,l = get_term_vectors(list_of_doc_ids)\n",
    "    term_frequency_dict = {}\n",
    "    return_list = []\n",
    "    for doc in v:\n",
    "        for t in v[doc]:\n",
    "            if t in term_frequency_dict.keys():\n",
    "                term_frequency_dict[t] = term_frequency_dict[t] + 1\n",
    "            else:\n",
    "                term_frequency_dict[t] = 1\n",
    "    for term in term_frequency_dict:\n",
    "        if term_frequency_dict[term] > len(list_of_doc_ids)-2:\n",
    "            return_list.append(term)\n",
    "    return return_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = 'fiber optics technology in use'\n",
    "tokens = get_tokens(query_text)\n",
    "doc_ids = get_relevant_docs(tokens)\n",
    "master_doc_dict, doc_length_dict = partition_docs(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1627,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ids = get_relevant_docs(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1632,
   "metadata": {},
   "outputs": [],
   "source": [
    "st\n",
    "st2 = 'allegations corrupt government    weather caused death prediction prime lending rate move'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1595,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_okapi_tf_pseudo_feedback(n, num_terms, master_doc_dict, doc_ids, doc_length_dict):\n",
    "    feedback_file = \"pseudo_feedback_output_file.txt\"\n",
    "    f_output = open(feedback_file, \"w+\")\n",
    "    file = open(\"AP_DATA/query_desc.51-100.short2.txt\", 'r')\n",
    "    \n",
    "    for line in file:\n",
    "        query_number = line.split('.')[0]\n",
    "        query_text = line[6:]\n",
    "        \n",
    "        tokens = get_tokens(query_text)\n",
    "        avg_doc_length = get_avg_doc_length()\n",
    "        \n",
    "        #doc_ids = get_relevant_docs(tokens)\n",
    "        #master_doc_dict, doc_length_dict = partition_docs(doc_ids)\n",
    "        # Get TF for documents\n",
    "        first_pass = get_ok_tf(tokens, master_doc_dict, doc_ids, doc_length_dict, avg_doc_length)\n",
    "        sorted_dict = sorted(first_pass.items(), key=operator.itemgetter(1), reverse=True)\n",
    " \n",
    "        # Only keep n documents\n",
    "        top_docs = sorted_dict[:n]\n",
    "        \n",
    "        # Find common terms\n",
    "        docs = []\n",
    "        for d in top_docs:\n",
    "            docs.append(d[0])\n",
    "        \n",
    "        terms = get_common_terms(docs)\n",
    "        res = [i for i in terms if i not in tokens]\n",
    "        print(res)\n",
    "        if num_terms > len(res):\n",
    "            m = len(res)\n",
    "        else:\n",
    "            m = num_terms\n",
    "        for i in range(0,m):\n",
    "            query_text = query_text + ' ' + res[i]\n",
    "        \n",
    "        print(query_text)\n",
    "        new_tokens = get_tokens(query_text)\n",
    "        #new_doc_ids = get_relevant_docs(new_tokens)\n",
    "        #new_master_doc_dict, new_doc_length_dict = partition_docs(new_doc_ids)\n",
    "        second_pass = get_ok_tf(new_tokens, master_doc_dict, doc_ids, doc_length_dict, avg_doc_length)\n",
    "        \n",
    "                    \n",
    "        write_to_file_okapi(query_number, second_pass, f_output)\n",
    "                \n",
    "    file.close()\n",
    "    f_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'govern', 'alleg', 'corrupt'}\n",
      "['alleg', 'corrupt', 'govern', 'alleg', 'corrupt']\n"
     ]
    }
   ],
   "source": [
    "test = 'allegations corrupt government    alleg corrupt'\n",
    "tok = get_tokens(test)\n",
    "print(set(tok))\n",
    "print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compani']\n",
      "fiber optics technology in use compani\n"
     ]
    }
   ],
   "source": [
    "get_okapi_tf_pseudo_feedback(9,2, master_doc_dict, doc_ids, doc_length_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC2: Pseudo-relevance Feedback Using ElasticSearch aggs \"significant terms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregations(term):\n",
    "    request = {\n",
    "        \"query\":{\n",
    "            \"terms\": {\n",
    "                \"text\":[term]\n",
    "            }\n",
    "        },\n",
    "        \"aggregations\": {\n",
    "            \"significantTerms\": {\n",
    "                \"significant_terms\": {\n",
    "                    \"field\":\"text\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"size\": 0\n",
    "    }   \n",
    "    related_words = []\n",
    "    res = es.search(index=index_name, body=request)['aggregations']['significantTerms']['buckets']\n",
    "    for r in res:\n",
    "        related_words.append(r['key'])\n",
    "    if len(related_words) > 0:\n",
    "        return related_words[1:]\n",
    "    else:\n",
    "        return related_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_to_add_to_query(query):\n",
    "    tokens = get_tokens(query)\n",
    "    term_map = {}\n",
    "    for token in tokens:\n",
    "        l = get_aggregations(token)\n",
    "        for word in l:\n",
    "            if word in term_map.keys():\n",
    "                term_map[word] = term_map[word] + 1\n",
    "            else:\n",
    "                term_map[word] = 1\n",
    "    sort = sorted(term_map.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_words_to_query(query):\n",
    "    l = get_words_to_add_to_query(query)\n",
    "    output = []\n",
    "    for word in l:\n",
    "        if word[1] > 1:\n",
    "            output.append(word[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['militari']"
      ]
     },
     "execution_count": 1461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_words_to_query(\"acquisition U.S. Army specified advanced weapons systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun models with modified queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1473,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_okapi_tf()\n",
    "#get_tf_idf()\n",
    "get_bm25()\n",
    "#get_unigram_laplace()\n",
    "#get_unigram_jelinek()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get list of all document ids, clear cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of all document ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch.helpers\n",
    "number_of_docs = es.count(index=index_name).get('count')\n",
    "request = {\n",
    "    \"query\":{\n",
    "        \"match_all\" : {}\n",
    "    }\n",
    "    ,'size': 10000\n",
    "}\n",
    "\n",
    "results = elasticsearch.helpers.scan(es,\n",
    "                                    index=index_name,preserve_order=True,query=request)\n",
    "ids = []\n",
    "for item in results:\n",
    "    ids.append(item['_id'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AP891220-0113',\n",
       " 'AP891220-0114',\n",
       " 'AP891220-0115',\n",
       " 'AP891220-0116',\n",
       " 'AP891220-0117',\n",
       " 'AP891220-0118',\n",
       " 'AP891220-0119',\n",
       " 'AP891220-0120',\n",
       " 'AP891220-0121',\n",
       " 'AP891220-0122']"
      ]
     },
     "execution_count": 1614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(ids))\n",
    "ids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_shards': {'total': 5, 'successful': 4, 'failed': 0}}"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Document will discuss allegations, or measures being taken against, corrupt public officials of any governmental jurisdiction worldwide. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = get_tokens(query)\n",
    "a = get_t1_bm25(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = get_doc_with_query_words(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42483"
      ]
     },
     "execution_count": 895,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
